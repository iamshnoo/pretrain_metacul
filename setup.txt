ml load gnu12/12.3.0
ml load python/3.12.1-33
python -m venv ~/pretrain-env
source ~/pretrain-env/bin/activate
pip install light-the-torch
ltt install torch==2.7.0 (pre-compiled wheel of flash attn is available for the combination of cuda 12.6 and python3.12 only for pytorch 2.7, for pytorch 2.8 cuda 12.8 or above would be required)
pip install transformers datasets huggingface-hub datasets safetensors accelerate numpy tqdm einops bitsandbytes trl peft lm-eval ninja packaging wandb
wget https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3+cu126torch2.7-cp312-cp312-linux_x86_64.whl
pip install flash_attn-2.6.3+cu126torch2.7-cp312-cp312-linux_x86_64.whl
rm flash_attn-2.6.3+cu126torch2.7-cp312-cp312-linux_x86_64.whl
pip install "cut-cross-entropy @ git+https://github.com/apple/ml-cross-entropy.git"
cd metacul
git clone https://github.com/samblouir/quick_llama
cd quick_llama
pip install -e .
cd ..
cd ..

cd pretrain_metacul/src

# Run this command to generate all train_configs files
python configs.py

# make sure you login to wandb
# make sure you have huggingface token set up for downloading private datasets

# Run this command to start pretraining with 4 A100 80GB GPUs on train_configs/continents/asia/without_metadata/pretraining.yaml
sbatch --gres=gpu:A100.80gb:4 --array=1-1 9_train_meco.slurm 4
